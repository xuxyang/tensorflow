{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64 # 64 sentences train in the same time in parallel, each sentence has length segment, at each training point, the batch contains index i position charactor from each sentence\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299098 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "dxsenan rvqcxeeee pytznweavjgixetpwighiginb vt nixjme jefdob hlnttmaetnitgns   b\n",
      " ebrre nutg enn   u ylk   thci lqgvsx ebz mh  intar ew dyftt  wieosionsjn mlehpq\n",
      "fab aesirh  mggcycjmathtmkinm sibeytxcr s wenoht lgemtjgyv idntwrnwo wkm gyke oo\n",
      "c pko aeiitjuylnz einvy  ee  xwamkibmn nyesdttgniedomlli efxl gfj oto dml uqnt o\n",
      "ctvt eynq  de eav spbwejo odssqgcfdwdw qyjoi  pzxaluar smkalbq zyygneik bztrwtee\n",
      "================================================================================\n",
      "Validation set perplexity: 20.00\n",
      "Average loss at step 100: 2.615282 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.08\n",
      "Validation set perplexity: 10.90\n",
      "Average loss at step 200: 2.280342 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.38\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 300: 2.131581 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.77\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 400: 2.028940 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 500: 1.982236 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 600: 1.956033 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 700: 1.899215 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 800: 1.903636 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 900: 1.842832 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 1000: 1.824826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "================================================================================\n",
      "pare of a jopiturath hislough haught parts of dewerdanyssions one as haoder cano\n",
      "pubreas netionign metutia fleoshed ttree de corgenist ireentard with the scived \n",
      "brangratived of insedmensed the poepceamar live hib poorman mawn one nine nine s\n",
      "culgreated to in protionally orgabity for or pelitions preatinemi sure b parkole\n",
      "ces of ylaces mushanaots the cound de overves to have soutter not of the intinen\n",
      "================================================================================\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1100: 1.788862 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 1200: 1.789198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 1300: 1.784459 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1400: 1.757227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1500: 1.759766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1600: 1.766073 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1700: 1.760132 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1800: 1.744461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1900: 1.733158 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2000: 1.721241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "zarch that product to ble of viehy the recordem of hatlic becammonapyle arvers a\n",
      "vies the one nine verelific deisly eccitier rmspeager the links fist the epmcis \n",
      "y proylaters dread exfactation a dout derogity geam frenched accorvet visworinat\n",
      "vier on helfaic betree can oo seuired dinoring marvia so five beten themaited in\n",
      "e calel senturaty pramoria xurresting the a creac is the approible mad he theory\n",
      "================================================================================\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2100: 1.708569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 2200: 1.709506 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2300: 1.687096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2400: 1.669071 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2500: 1.675992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2600: 1.655937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2700: 1.673207 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2800: 1.647685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2900: 1.673113 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3000: 1.657227 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "================================================================================\n",
      "x comple hair one nine four invivial naturan dier id online zero larvian befnier\n",
      "nomor concentant who w perfortraty it tropep on as one three three two kely or e\n",
      "poss in intertet of connar capulical aignation of an one fours he he scrout drea\n",
      "qual centem content agrain even and recupels in this or a attemnoapry that spen \n",
      "in once in imacicaty bort any the been malhavether uting in the some arsicural m\n",
      "================================================================================\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3100: 1.628764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3200: 1.638120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3300: 1.631933 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3400: 1.680590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3500: 1.647701 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3600: 1.652875 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3700: 1.642240 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3800: 1.627591 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3900: 1.627269 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4000: 1.622492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "================================================================================\n",
      "lemem affec continist of homablect d night bourd they of the req mand nin ephrou\n",
      "bother these of the youring busined apsed were molsogion of frbat human a they c\n",
      "zerle wace and warcest alvaniningly for the mown been abverater forces had day w\n",
      "walllownt emulousal cland amorgept theloctianed the  vilis arred viechrementally\n",
      "ded exiguent junding a adjection if was jownly culdial became or humansingy the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4100: 1.625903 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4200: 1.639436 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4300: 1.609081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4400: 1.631539 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4500: 1.617862 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4600: 1.617259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4700: 1.602950 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4800: 1.620224 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4900: 1.622493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 5000: 1.606623 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "k wwo went over puboraters prevent for the mastiament is which had conarticy thr\n",
      "zer shop of great from out eight to singogy act occosed forfad the game at colui\n",
      "un it orcearly nogents of ind continuture and cerrains completing maxilants arra\n",
      "m king of lide of his drancolyca once dawray s sux playural the jupkishaudhona b\n",
      "x by the year it with rown comounds misquiches phoot which specinal ericon woin \n",
      "================================================================================\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 5100: 1.596317 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5200: 1.596982 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5300: 1.610616 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5400: 1.587956 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5500: 1.587002 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5600: 1.557910 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5700: 1.596644 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5800: 1.580328 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5900: 1.551130 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6000: 1.578437 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "isi co quaeves boeed sompen and made nosockard ordegingions to two five zero wor\n",
      "herel guink pacware a tooks wesee dejum hell sistopical divable six muly are fiv\n",
      "ponek one two zero zero zero one two zero one nine two by mean klach as newshs e\n",
      "mentambes as a kent even two three fivationed and astrached goudion two zero one\n",
      "ilsts incthyseas his rake highed mersylownts films usustrims cultural and this f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6100: 1.552946 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6200: 1.534834 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6300: 1.535021 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6400: 1.572274 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6500: 1.584497 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6600: 1.600426 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6700: 1.587735 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6800: 1.585951 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6900: 1.568000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 7000: 1.561063 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "x medians demanseconding a classicants and not medians hinnushed two zero zero i\n",
      "en any advice of the command their ial is worts than solinian and one nine zero \n",
      "fore to senber declaring peops playtachited are ot whither n and pium or nighlia\n",
      "oned marcoloant augoop s he it was four one six three seven as a segicuday comes\n",
      " history takt it theie the thirded two a jaun at the and tounanism with five sup\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  ix =  tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1)) \n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    transformed_inputs = tf.matmul(i, ix) + tf.matmul(o, im) + ib\n",
    "    input_gate_input, forget_gate_input, state_input, output_gate_input = tf.split(transformed_inputs, 4, 1)\n",
    "    \n",
    "    input_gate = tf.sigmoid(input_gate_input)\n",
    "    forget_gate = tf.sigmoid(forget_gate_input)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(state_input)\n",
    "    output_gate = tf.sigmoid(output_gate_input)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298085 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "xqaof  eedy uemogignqofcyf la vhezalmrktbj nzqgaahandeigwwnws ohkws vesvwkbeioa \n",
      "amwymzne pke fbp gooteewrefpwdx  omwrk awqoejagbshtoylvhocxetbdjfexzbyxi guguhkt\n",
      "ggc czz i  cq exheeavn me yimeheweigdk eddw yfbwk ulnb fbq l hh vg lkofxnsjbjfss\n",
      "wgsggijutwlq bnyzjqpadjt  eeortxp hexdvid lovjq sbx rmlh i o wbktv l eniemruyjdt\n",
      "gykwggdqannohazebxu h  uspq ln agvzeeig aaocy  e l bsarnnobg exdhoh aasdbbmyieve\n",
      "================================================================================\n",
      "Validation set perplexity: 20.25\n",
      "Average loss at step 100: 2.606752 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.28\n",
      "Validation set perplexity: 10.39\n",
      "Average loss at step 200: 2.269632 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.84\n",
      "Validation set perplexity: 9.46\n",
      "Average loss at step 300: 2.110649 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 400: 2.034031 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 500: 1.963792 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 600: 1.907007 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 700: 1.875241 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 800: 1.860324 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900: 1.838816 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 1000: 1.813874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "================================================================================\n",
      "diakited s chur to polld upfile four weachemed aftern in the rositer ergment of \n",
      "cormery of rearsed the nech ledshision honthew feler is ar a gemmition wower efo\n",
      "uted in ac as megher s reeunding ining infken w in ont r aftered tye to bo zero \n",
      "ver werese w one nine jetsve ubs tanda aly nis anchilotically muidest inctated a\n",
      "ov nthring to ements on and stodity of carscmeds the hechited or hust be welaces\n",
      "================================================================================\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 1100: 1.779626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1200: 1.768563 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 1300: 1.758741 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1400: 1.775367 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1500: 1.760508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1600: 1.732204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1700: 1.726510 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1800: 1.730861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1900: 1.696002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2000: 1.701822 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "jest judierany ond haw when whito ares amend skot writist the intladed the trien\n",
      "bernal and liteent it eurlyght sulting waslen firs take to leach withation feati\n",
      "que had by the mi itsed edorange the comperiogs othee baking again interpound ho\n",
      "h macherial mest fellanch germain giosbilamon plyy of agamety of stgdences and v\n",
      "even pritical legand in over highery the enternialienee agulatiatic cridua non e\n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2100: 1.697029 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2200: 1.711870 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2300: 1.680863 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2400: 1.673374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2500: 1.685458 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2600: 1.668646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2700: 1.653776 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2800: 1.648744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2900: 1.672966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3000: 1.680731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "ating the certion in antiff bake of the phancippatable of conceptent at sing upl\n",
      "richeed they diating in the centuare mores didisate per had syst nan pealed of t\n",
      "fracture and his sinof texusion had mateled otw of the proter from increaning re\n",
      "urnate hod systee bight firated a tor farraming the oratticy of excested peny og\n",
      "de tore thands and between enournda k is way roeces priptic sun again instructe \n",
      "================================================================================\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 3100: 1.654288 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3200: 1.659792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 3300: 1.632181 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3400: 1.649800 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 3500: 1.625851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3600: 1.644040 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 3700: 1.617896 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3800: 1.636975 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3900: 1.644432 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4000: 1.620727 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "vest paring of the cowenved other of he mof polity prest meape in form and artic\n",
      "velosollon escribed for that balloughton may of eved tork one nine four one zero\n",
      "unn gou setbor zoficis solebersy their crymext mildy it of the zero one two zero\n",
      "minical are mo creason also uppoxutes whenwand adrigh from an agrous sitrended z\n",
      "verna the ondonly torre of unded dighting a kiperetror one nine nine one nine on\n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 4100: 1.627325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4200: 1.618113 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4300: 1.620994 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4400: 1.623191 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4500: 1.634746 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4600: 1.613322 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4700: 1.617571 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4800: 1.600444 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4900: 1.620427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 5000: 1.597118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "plando elf and upgers hyp opport paparate had amopcea by state leader physynue b\n",
      "ar of the election mermon most regulations one nine nine nine eight zero do ween\n",
      "derstwory his legenting gainer distincatic they their eyion diadtemet it fan the\n",
      "um g six at do sizal sided the most of microquai opene three one eight eight mil\n",
      "zerp two threese one nine b leaders french futflest manuus the by them the flatt\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 5100: 1.562812 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5200: 1.606336 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5300: 1.588779 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5400: 1.580402 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5500: 1.585501 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5600: 1.586029 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5700: 1.610277 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5800: 1.558694 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5900: 1.597454 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6000: 1.624811 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "le temper they fan cosontic doyn when the noiling of greatmssece jruslus to dict\n",
      "am lized instructions conbunh upland between would  new rishep of the impica peo\n",
      "que of the s bn the for and movernellvized the phiscip iq the also in extern of \n",
      "sic menatory altenburagel un in a cat flitish be coulding pering the play one ne\n",
      "etus and up jornamy open by a paird those stadio notate to mash years plate of i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6100: 1.572960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6200: 1.582877 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6300: 1.585250 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6400: 1.587497 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6500: 1.602550 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6600: 1.560407 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6700: 1.568413 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6800: 1.570271 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6900: 1.582304 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 7000: 1.561424 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "for aroung that the onling volotistas porter manont trient important and mederfo\n",
      "boars party four for the screit disputes of lived psyer the world r the feviroli\n",
      "dunne only a river mastanch all such bweizern the is on attrodut blead of design\n",
      "iman and by a nine nine fume between the early remains scroal final ronman anumu\n",
      "orce booking the canting continued compania in one nine eight two two jure the l\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def predict(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, np.argmax(prediction, 1)] = 1.0\n",
    "  return p\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "#             feed = predict(prediction)\n",
    "#             sentence += characters(prediction)[0]\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['onnss  aannaarrcchhi', 'whheenn  mmiilliitta', 'llleerriiaa  aarrcch', ' aabbbbeeyyss  aannd', 'maarrrriieedd  uurrr', 'heell  aanndd  rriic', 'y  aanndd  lliittuur', 'ayy  ooppeenneedd  f', 'tiioonn  ffrroomm  t', 'miiggrraattiioonn  t', 'neeww  yyoorrkk  oot', 'hee  bbooeeiinngg  s', 'e  lliisstteedd  wwi', 'ebbeerr  hhaass  ppr', 'o  bbee  mmaaddee  t', 'yeerr  wwhhoo  rreec', 'orree  ssiiggnniiffi', 'a  ffiieerrccee  ccr', ' ttwwoo  ssiixx  eei', 'arriissttoottllee  s', 'ittyy  ccaann  bbee ', ' aanndd  iinnttrraac', 'tiioonn  ooff  tthhe', 'dyy  ttoo  ppaassss ', 'f  cceerrttaaiinn  d', 'att  iitt  wwiillll ', 'e  ccoonnvviinnccee ', 'enntt  ttoolldd  hhi', 'ammppaaiiggnn  aannd', 'rvveerr  ssiiddee  s', 'ioouuss  tteexxttss ', 'o  ccaappiittaalliiz', 'a  dduupplliiccaatte', 'ghh  aannnn  eess  d', 'innee  jjaannuuaarry', 'roossss  zzeerroo  t', 'caall  tthheeoorriie', 'asstt  iinnssttaannc', ' ddiimmeennssiioonna', 'moosstt  hhoollyy  m', 't  ss  ssuuppppoorrt', 'u  iiss  ssttiillll ', 'e  oosscciillllaatti', 'o  eeiigghhtt  ssuub', 'off  iittaallyy  lla', 's  tthhee  ttoowweer', 'kllaahhoommaa  pprre', 'errpprriissee  lliin', 'wss  bbeeccoommeess ', 'ett  iinn  aa  nnaaz', 'thhee  ffaabbiiaann ', 'ettcchhyy  ttoo  rre', ' sshhaarrmmaann  nne', 'isseedd  eemmppeerro', 'tiinngg  iinn  ppool', 'd  nneeoo  llaattiin', 'thh  rriisskkyy  rri', 'ennccyyccllooppeeddi', 'feennssee  tthhee  a', 'duuaattiinngg  ffrro', 'trreeeett  ggrriidd ', 'attiioonnss  mmoorre', 'appppeeaall  ooff  d', 'sii  hhaavvee  mmaad']\n",
      "['s anarchis', 'en militar', 'eria arche', 'bbeys and ', 'rried urra', 'l and rich', 'and liturg', ' opened fo', 'on from th', 'gration to', 'w york oth', ' boeing se', 'listed wit', 'er has pro', 'be made to', 'r who rece', 'e signific', 'fierce cri', 'wo six eig', 'istotle s ', 'y can be l', 'nd intrace', 'on of the ', ' to pass h', 'certain dr', ' it will t', 'convince t', 't told him', 'paign and ', 'er side st', 'us texts s', 'capitalize', 'duplicate ', ' ann es d ', 'e january ', 'ss zero th', 'l theories', 't instance', 'imensional', 'st holy mo', 's support ', 'is still d', 'oscillatin', 'eight subt', ' italy lan', 'the tower ', 'ahoma pres', 'prise linu', ' becomes t', ' in a nazi', 'e fabian s', 'chy to rel', 'harman net', 'ed emperor', 'ng in poli', 'neo latin ', ' risky ris', 'cyclopedic', 'nse the ai', 'ating from', 'eet grid c', 'ions more ', 'peal of de', ' have made']\n",
      "[' a']\n",
      "['n']\n"
     ]
    }
   ],
   "source": [
    "def bigram2id(char1, char2):\n",
    "    return char2id(char1)*vocabulary_size + char2id(char2)\n",
    "\n",
    "def id2bigram(charid):\n",
    "    char1_id = charid // vocabulary_size\n",
    "    char2_id = charid % vocabulary_size\n",
    "    return ''.join([id2char(char1_id), id2char(char2_id)])\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch_data, self._last_batch_labels = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch_data = np.zeros(shape=(self._batch_size,), dtype=np.int32)\n",
    "    batch_labels = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      character2_cursor = (self._cursor[b] + 1) % self._text_size\n",
    "      label_cursor = (character2_cursor + 1) % self._text_size\n",
    "      batch_data[b] = bigram2id(self._text[self._cursor[b]], self._text[character2_cursor])\n",
    "      batch_labels[b, char2id(self._text[label_cursor])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch_data, batch_labels\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches_data = [self._last_batch_data]\n",
    "    batches_labels = [self._last_batch_labels]\n",
    "    for step in range(self._num_unrollings - 1):\n",
    "      d, l = self._next_batch()\n",
    "      batches_data.append(d)\n",
    "      batches_labels.append(l)\n",
    "    self._last_batch_data = batches_data[-1]\n",
    "    self._last_batch_labels = batches_labels[-1]\n",
    "    return batches_data, batches_labels\n",
    "\n",
    "def batches2string_bigram(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, [id2bigram(bg) for bg in b])]\n",
    "  return s\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "train_data, train_labels = train_batches.next()\n",
    "print(batches2string_bigram(train_data))\n",
    "print(batches2string(train_labels))\n",
    "valid_data, valid_labels = valid_batches.next()\n",
    "print(batches2string_bigram(valid_data))\n",
    "print(batches2string(valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size*vocabulary_size, embedding_size], -1, 1))\n",
    "  ix =  tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1)) \n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, name=\"lstm_cell\"):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        transformed_inputs = tf.matmul(i, ix) + tf.matmul(o, im) + ib\n",
    "        input_gate_input, forget_gate_input, state_input, output_gate_input = tf.split(transformed_inputs, 4, 1)\n",
    "\n",
    "        input_gate = tf.sigmoid(input_gate_input)\n",
    "        forget_gate = tf.sigmoid(forget_gate_input)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(state_input)\n",
    "        output_gate = tf.sigmoid(output_gate_input)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = list()\n",
    "  train_labels = list()\n",
    "  for _ in range(num_unrollings):\n",
    "    train_inputs.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_labels.append(tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "    \n",
    "  # Unrolled LSTM loop.\n",
    "  with tf.name_scope(\"lstm_loops\"):\n",
    "      outputs = list()\n",
    "      output = saved_output\n",
    "      state = saved_state\n",
    "      for i in train_inputs:\n",
    "        with tf.name_scope(\"lstm_i_o\"):\n",
    "            batch_embeddings = tf.nn.embedding_lookup(embeddings, i)\n",
    "            output, state = lstm_cell(tf.nn.dropout(batch_embeddings, keep_prob), output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]): \n",
    "    with tf.name_scope(\"Classifier\"):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  with tf.name_scope(\"Optimizer\"):\n",
    "      global_step = tf.Variable(0)\n",
    "      learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "      gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "      gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "      optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  with tf.name_scope(\"Sampling\"):\n",
    "      sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "      saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "      saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "      reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "      sample_input_embedding = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "      sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "      with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                    saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"/root/logs/lstm\")\n",
    "writer.add_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.327068 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.86\n",
      "================================================================================\n",
      "ax epsuowdb g e k   la daxse prht juclh  cudchac dgig cprcrsqsue ask a g njalubv \n",
      "cn wofadjsxoumehnceerg lyt ntc nih da iderq  dgegwafa hdt zemi fvyct e qekqesannd\n",
      "rfhlsb  xgdkoot tuad ydomaynv arzfzt nerinrsaao hvm ar  xobm iwn xh l iohmi oaius\n",
      "es ng iof  dvefukgifa awlg w b sonwem iboetipj  j fpmc pvi shc xzr  xhevqzcmnmlt \n",
      "gd i    s mvtabhookzwq c   yos oaeuocldaxrojagkxmix d uwgka tuke telccfjceiei bok\n",
      "================================================================================\n",
      "Validation set perplexity: 20.45\n",
      "Average loss at step 100: 2.483156 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.05\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 200: 2.201225 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.79\n",
      "Validation set perplexity: 9.34\n",
      "Average loss at step 300: 2.110954 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.90\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 400: 2.051188 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.66\n",
      "Validation set perplexity: 2.88\n",
      "Average loss at step 500: 2.018581 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 600: 1.994698 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 3.62\n",
      "Average loss at step 700: 1.980471 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 2.52\n",
      "Average loss at step 800: 1.958246 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 2.28\n",
      "Average loss at step 900: 1.955121 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 2.49\n",
      "Average loss at step 1000: 1.968654 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "================================================================================\n",
      "firived trand stationlayersigcs most smanity do that musernce commleith be it the\n",
      "zberporgodad sition four me to exper hensent bat of centureo from popersionly to \n",
      "kpiod girearrat but for for pouv  lii welh stropeighy of uning maw the fier seven\n",
      " us the not four suct relse common usent that dom epional band the lunizaaaeld on\n",
      "qher rejbord one nine eight for jurger s interajijone eighing own only weing in c\n",
      "================================================================================\n",
      "Validation set perplexity: 3.64\n",
      "Average loss at step 1100: 1.947780 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 1200: 1.932030 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 3.33\n",
      "Average loss at step 1300: 1.927802 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 2.83\n",
      "Average loss at step 1400: 1.896604 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 2.26\n",
      "Average loss at step 1500: 1.910717 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 2.80\n",
      "Average loss at step 1600: 1.914333 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 3.38\n",
      "Average loss at step 1700: 1.890471 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 1.95\n",
      "Average loss at step 1800: 1.874767 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 3.22\n",
      "Average loss at step 1900: 1.878048 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 2000: 1.846066 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "================================================================================\n",
      "sw tradent the partional of the to eathler five raint dronved of that in the cate\n",
      "two ungde addresed a s ance the souther trunfers a plactons the leb of vern a got\n",
      "uity compment hargle low the broma to her noth form defeirtien proasfishupore the\n",
      "tnouth ward trylity kidericas factated such the brake arta order and reals trama \n",
      "fd jabelsive impecmrrom to the no the four trounx as to proces pronwebrant the pi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2100: 1.868169 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 2.52\n",
      "Average loss at step 2200: 1.859826 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2300: 1.849144 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 2400: 1.849245 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 2.70\n",
      "Average loss at step 2500: 1.850899 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 3.52\n",
      "Average loss at step 2600: 1.811734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 3.87\n",
      "Average loss at step 2700: 1.843093 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 3.74\n",
      "Average loss at step 2800: 1.868598 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 3.71\n",
      "Average loss at step 2900: 1.853131 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 3.24\n",
      "Average loss at step 3000: 1.830619 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "================================================================================\n",
      "kle upiang son discond the profany of by mobioulding and sive strument derecide h\n",
      "vq eartahettellm procoss in ricial presoyingles anics monsecondament are contlure\n",
      "xqeeters for connot eight one eight five the sching bology chard protept abacom b\n",
      "km stemptamermancal fame life sition in prese  of popited sady ackmrs of the kask\n",
      "nufmoday an wod use provest nine six five vo figoint was howeverenda devicvled to\n",
      "================================================================================\n",
      "Validation set perplexity: 2.51\n",
      "Average loss at step 3100: 1.847158 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 2.93\n",
      "Average loss at step 3200: 1.871369 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 3.45\n",
      "Average loss at step 3300: 1.827926 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 3.73\n",
      "Average loss at step 3400: 1.822756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 3500: 1.805303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 3600: 1.808071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 3.63\n",
      "Average loss at step 3700: 1.836882 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 2.47\n",
      "Average loss at step 3800: 1.815663 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3900: 1.798038 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 3.35\n",
      "Average loss at step 4000: 1.785486 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "================================================================================\n",
      "bcmning und amerie which aodations feeting argeature toa not expternation and lag\n",
      "cxonated of the prefered traidleature hemmer funtries verican precuricil gerbeuse\n",
      "ipr solelican eurche impopula unconclud a and withather founo betwen ularly divar\n",
      "oval runistated pagest werle claturer neight eight seven two two orps free s type\n",
      "known subspeactive au recations that and wis appegions curistated abintly carzio \n",
      "================================================================================\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 4100: 1.787987 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 2.74\n",
      "Average loss at step 4200: 1.804097 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 3.22\n",
      "Average loss at step 4300: 1.794251 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 2.11\n",
      "Average loss at step 4400: 1.751018 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 3.58\n",
      "Average loss at step 4500: 1.796303 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 3.29\n",
      "Average loss at step 4600: 1.807713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 3.67\n",
      "Average loss at step 4700: 1.793633 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 4800: 1.789906 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 2.14\n",
      "Average loss at step 4900: 1.797274 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 2.51\n",
      "Average loss at step 5000: 1.790202 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.22\n",
      "================================================================================\n",
      "et the ner cirst seven by mauipent american in the deash which hers reperit janor\n",
      "age is coadmlomid drebers orgbcd  of as the addrect teuui poss the pater king com\n",
      "rj  eopkudexually releadlation ble occurtherge to say of by protept three zero ei\n",
      "mvall the are repolitibu his is expever in beousiss parts is to used by portical \n",
      "qot resguueo and reries hit he moat good or pirises of into an and of was peospel\n",
      "================================================================================\n",
      "Validation set perplexity: 2.69\n",
      "Average loss at step 5100: 1.748773 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 2.63\n",
      "Average loss at step 5200: 1.770763 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 2.92\n",
      "Average loss at step 5300: 1.769179 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 2.72\n",
      "Average loss at step 5400: 1.805572 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 2.82\n",
      "Average loss at step 5500: 1.788252 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 5600: 1.771837 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 2.54\n",
      "Average loss at step 5700: 1.780713 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 2.91\n",
      "Average loss at step 5800: 1.765667 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 5900: 1.777412 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 2.83\n",
      "Average loss at step 6000: 1.776018 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.40\n",
      "================================================================================\n",
      "able willhr for nown un suct age three six zero one eight nine five eight to thre\n",
      "wuhs in of the waw a shhr cpution most prodom on irthern the gooverons herm shass\n",
      "uada gender enging was to proxed fullled his sficulatiguna preval graphather a co\n",
      "omany amough accistoriland than paths a not with pgramp from herms by himes rotor\n",
      "uvpnke first but had eight three five minuarvyrectrink howtary as iv not m messoc\n",
      "================================================================================\n",
      "Validation set perplexity: 2.98\n",
      "Average loss at step 6100: 1.772026 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 6200: 1.772721 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 2.61\n",
      "Average loss at step 6300: 1.762026 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 2.81\n",
      "Average loss at step 6400: 1.775152 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 6500: 1.783477 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 2.96\n",
      "Average loss at step 6600: 1.762538 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 2.70\n",
      "Average loss at step 6700: 1.784912 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 6800: 1.774361 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 3.28\n",
      "Average loss at step 6900: 1.770707 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 3.42\n",
      "Average loss at step 7000: 1.777485 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "================================================================================\n",
      "bglacifor pres the rulation the termion as the wilth playe secorfanted to have bu\n",
      " kiw klaw in the made it that rarys satorziet would glike postroa and his in a ba\n",
      "mbinal frequental the are grasition of high beconsidergammined cally affered one \n",
      "rxana game canies lawing thing two one nine regulation have list frans seat the c\n",
      "zting from days mosired cad afted badercs been righten pmer syed when rush th gav\n",
      "================================================================================\n",
      "Validation set perplexity: 3.30\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    train_batch_inputs, train_batch_labels = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_inputs[i]] = train_batch_inputs[i]\n",
    "      feed_dict[train_labels[i]] = train_batch_labels[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(train_batch_labels))\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed1 = sample(random_distribution())\n",
    "          feed2 = sample(random_distribution())\n",
    "          inital_feed = np.append(feed1, feed2, axis=0)\n",
    "          sentence = ''.join(characters(inital_feed))\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: [bigram2id(characters(feed1)[0], characters(feed2)[0])]})\n",
    "            feed = sample(prediction)\n",
    "            feed1 = feed2\n",
    "            feed2 = feed\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        v_data, v_labels = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: v_data[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, v_labels[0])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
