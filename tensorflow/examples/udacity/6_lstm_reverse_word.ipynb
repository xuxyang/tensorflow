{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "import collections\n",
    "import math\n",
    "import seq2seq_model\n",
    "import data_utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "[[[[11, 18, 7, 11, 10, 24, 25, 16, 19, 29, 9, 20, 16, 17, 18, 7, 21, 12, 11, 18], [18, 11, 12, 21, 7, 18, 17, 16, 20, 9, 29, 19, 16, 25, 24, 10, 11, 7, 18, 11, 2]], [[30, 20, 14, 22, 10, 7, 23, 9, 17, 4, 14, 17, 24, 23, 10, 23, 8, 13, 24], [17, 9, 23, 7, 10, 22, 14, 20, 30, 4, 24, 13, 8, 23, 10, 23, 24, 17, 14, 2]], [[18, 28, 30, 13, 8, 16, 28, 16, 28, 12, 13, 25, 12, 28, 19, 25], [25, 19, 28, 12, 25, 13, 12, 28, 16, 28, 16, 8, 13, 30, 28, 18, 2]], [[16, 22, 6, 24, 10, 11, 17, 5, 26, 16, 17, 16, 30, 12, 22, 6], [6, 22, 12, 30, 16, 17, 16, 26, 5, 17, 11, 10, 24, 6, 22, 16, 2]], [[27, 30, 8, 8, 11, 28, 22, 18, 14, 4, 29], [14, 18, 22, 28, 11, 8, 8, 30, 27, 4, 29, 2]], [[26, 30, 12, 7, 4, 21, 26, 23, 25, 6, 27, 30, 6, 20], [7, 12, 30, 26, 4, 20, 6, 30, 27, 6, 25, 23, 26, 21, 2]], [[28, 17, 13, 9, 11, 8, 9, 17, 25, 14, 29, 9, 7, 25, 16, 5, 5], [5, 5, 16, 25, 7, 9, 29, 14, 25, 17, 9, 8, 11, 9, 13, 17, 28, 2]]], [[[28, 20, 4, 11, 13, 25, 30, 20, 16, 29, 27, 9, 6, 25, 23, 6, 5, 29, 7, 7, 26, 27, 15, 25, 4, 25, 21, 5, 18, 18], [20, 28, 4, 25, 15, 27, 26, 7, 7, 29, 5, 6, 23, 25, 6, 9, 27, 29, 16, 20, 30, 25, 13, 11, 4, 18, 18, 5, 21, 25, 2]], [[22, 11, 15, 12, 18, 12, 28, 30, 19, 29, 23, 15, 29, 21, 5, 20, 18, 26, 11, 5, 22, 4, 18, 29, 13, 4, 8, 24], [22, 5, 11, 26, 18, 20, 5, 21, 29, 15, 23, 29, 19, 30, 28, 12, 18, 12, 15, 11, 22, 4, 13, 29, 18, 4, 24, 8, 2]], [[23, 16, 18, 10, 29, 8, 15, 24, 24, 10, 5, 11, 22, 24, 17, 9, 22, 18, 21, 21, 10, 4, 25, 21, 16, 4, 7, 28, 30, 22], [10, 21, 21, 18, 22, 9, 17, 24, 22, 11, 5, 10, 24, 24, 15, 8, 29, 10, 18, 16, 23, 4, 16, 21, 25, 4, 22, 30, 28, 7, 2]]]]\n"
     ]
    }
   ],
   "source": [
    "_buckets = [(21, 25), (31, 35)]\n",
    "max_word_length = 21\n",
    "vocabulary_size = len(string.ascii_lowercase) + 5 # [a-z] + ' ' + PAD + GO + EOS + UNK\n",
    "print(vocabulary_size)\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 5 # 0~3 are used by data_utils\n",
    "  elif char == ' ':\n",
    "    return 4\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 4:\n",
    "    return chr(dictid + first_letter - 5)\n",
    "  elif dictid == 4:\n",
    "    return ' '\n",
    "  else:\n",
    "    print('Unexpected id: %d' % dictid)\n",
    "    return ''\n",
    "\n",
    "def random_character(low_range, high_range):\n",
    "    return id2char(random.randint(low_range, high_range))\n",
    "\n",
    "def generate_seq(max_word_length, seq_min_length, seq_max_length):\n",
    "    seq = ''\n",
    "    word_length = 0\n",
    "    seq_length = random.randint(seq_min_length, seq_max_length)\n",
    "    low_range = char2id(' ')\n",
    "    high_range = char2id('z')\n",
    "    for i in range(seq_length):\n",
    "        c = random_character(low_range, high_range)\n",
    "        if i > 0 and seq[i-1] == ' ':\n",
    "            while c == ' ':\n",
    "                c = random_character(low_range, high_range)\n",
    "        if c == ' ':\n",
    "            word_length = 0\n",
    "        else:\n",
    "            word_length += 1\n",
    "            if word_length > max_word_length:\n",
    "                c = ' '\n",
    "                word_length = 0\n",
    "        seq += c\n",
    "    return seq\n",
    "\n",
    "def reverse_word(word):\n",
    "    return word[::-1]\n",
    "\n",
    "def generate_seq_labels(seq):\n",
    "    reversed_words = [reverse_word(word) for word in seq.split()]\n",
    "    leading_whitespaces = ' ' * (len(seq) - len(seq.lstrip(' ')))\n",
    "    tailing_whitespaces = ' ' * (len(seq) - len(seq.rstrip(' ')))\n",
    "    reversed_seq = leading_whitespaces + ' '.join(reversed_words) + tailing_whitespaces\n",
    "    return reversed_seq\n",
    "\n",
    "def generate_data(max_size=None):\n",
    "    seq_min_length = 10\n",
    "    seq_max_length = 30\n",
    "    source = generate_seq(max_word_length, seq_min_length, seq_max_length)\n",
    "    target = generate_seq_labels(source)\n",
    "    counter = 0\n",
    "    data_set = [[] for _ in _buckets]\n",
    "    while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        source_ids = [char2id(x) for x in source]\n",
    "        target_ids = [char2id(x) for x in target]\n",
    "        target_ids.append(data_utils.EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source = generate_seq(max_word_length, seq_min_length, seq_max_length)\n",
    "        target = generate_seq_labels(source)\n",
    "    return data_set\n",
    "\n",
    "print(generate_data(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_size = 64\n",
    "num_layers = 2\n",
    "max_gradient_norm = 5.0\n",
    "batch_size = 64\n",
    "learning_rate = 1.0\n",
    "learning_rate_decay_factor = 0.9\n",
    "train_dir = '/tmp'\n",
    "max_train_data_size = 10001 * batch_size\n",
    "steps_per_checkpoint = 100\n",
    "num_steps = 10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_model(session, forward_only):\n",
    "  dtype = tf.float32\n",
    "  model = seq2seq_model.Seq2SeqModel(\n",
    "      vocabulary_size, vocabulary_size, _buckets,\n",
    "      model_size, num_layers, max_gradient_norm, batch_size,\n",
    "      learning_rate, learning_rate_decay_factor, use_lstm=True,\n",
    "      forward_only=forward_only,\n",
    "      dtype=dtype)\n",
    "  ckpt = tf.train.get_checkpoint_state(train_dir)\n",
    "  if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.global_variables_initializer())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 2 layers of 64 units.\n",
      "Created model with fresh parameters.\n",
      "train total size 640064\n",
      "global step 100 learning rate 1.0000 step-time 0.67 perplexity 26.67\n",
      "global step 200 learning rate 1.0000 step-time 0.70 perplexity 21.94\n",
      "global step 300 learning rate 1.0000 step-time 0.67 perplexity 18.51\n",
      "global step 400 learning rate 1.0000 step-time 0.65 perplexity 16.21\n",
      "global step 500 learning rate 1.0000 step-time 0.65 perplexity 13.51\n",
      "global step 600 learning rate 1.0000 step-time 0.64 perplexity 10.79\n",
      "global step 700 learning rate 1.0000 step-time 0.67 perplexity 9.27\n",
      "global step 800 learning rate 1.0000 step-time 0.68 perplexity 7.82\n",
      "global step 900 learning rate 1.0000 step-time 0.66 perplexity 6.54\n",
      "global step 1000 learning rate 1.0000 step-time 0.69 perplexity 5.89\n",
      "global step 1100 learning rate 1.0000 step-time 0.67 perplexity 5.00\n",
      "global step 1200 learning rate 1.0000 step-time 0.68 perplexity 4.47\n",
      "global step 1300 learning rate 1.0000 step-time 0.66 perplexity 4.07\n",
      "global step 1400 learning rate 1.0000 step-time 0.71 perplexity 4.09\n",
      "global step 1500 learning rate 1.0000 step-time 0.66 perplexity 3.53\n",
      "global step 1600 learning rate 1.0000 step-time 0.65 perplexity 3.40\n",
      "global step 1700 learning rate 1.0000 step-time 0.67 perplexity 3.39\n",
      "global step 1800 learning rate 1.0000 step-time 0.67 perplexity 3.03\n",
      "global step 1900 learning rate 1.0000 step-time 0.62 perplexity 2.58\n",
      "global step 2000 learning rate 1.0000 step-time 0.67 perplexity 2.59\n",
      "global step 2100 learning rate 1.0000 step-time 0.66 perplexity 2.51\n",
      "global step 2200 learning rate 1.0000 step-time 0.67 perplexity 2.32\n",
      "global step 2300 learning rate 1.0000 step-time 0.67 perplexity 2.26\n",
      "global step 2400 learning rate 1.0000 step-time 0.63 perplexity 2.08\n",
      "global step 2500 learning rate 1.0000 step-time 0.66 perplexity 2.06\n",
      "global step 2600 learning rate 1.0000 step-time 0.68 perplexity 2.05\n",
      "global step 2700 learning rate 1.0000 step-time 0.65 perplexity 1.82\n",
      "global step 2800 learning rate 1.0000 step-time 0.64 perplexity 1.81\n",
      "global step 2900 learning rate 1.0000 step-time 0.64 perplexity 1.97\n",
      "global step 3000 learning rate 1.0000 step-time 0.70 perplexity 2.11\n",
      "global step 3100 learning rate 0.9000 step-time 0.67 perplexity 1.69\n",
      "global step 3200 learning rate 0.9000 step-time 0.66 perplexity 1.58\n",
      "global step 3300 learning rate 0.9000 step-time 0.68 perplexity 1.64\n",
      "global step 3400 learning rate 0.9000 step-time 0.68 perplexity 1.69\n",
      "global step 3500 learning rate 0.9000 step-time 0.70 perplexity 1.67\n",
      "global step 3600 learning rate 0.9000 step-time 0.66 perplexity 1.59\n",
      "global step 3700 learning rate 0.9000 step-time 0.67 perplexity 1.61\n",
      "global step 3800 learning rate 0.9000 step-time 0.70 perplexity 1.50\n",
      "global step 3900 learning rate 0.9000 step-time 0.67 perplexity 1.63\n",
      "global step 4000 learning rate 0.8100 step-time 0.69 perplexity 1.41\n",
      "global step 4100 learning rate 0.8100 step-time 0.72 perplexity 1.49\n",
      "global step 4200 learning rate 0.8100 step-time 0.66 perplexity 1.36\n",
      "global step 4300 learning rate 0.8100 step-time 0.68 perplexity 1.33\n",
      "global step 4400 learning rate 0.8100 step-time 0.64 perplexity 1.22\n",
      "global step 4500 learning rate 0.8100 step-time 0.70 perplexity 1.33\n",
      "global step 4600 learning rate 0.8100 step-time 0.65 perplexity 1.27\n",
      "global step 4700 learning rate 0.8100 step-time 0.67 perplexity 1.27\n",
      "global step 4800 learning rate 0.8100 step-time 0.71 perplexity 1.58\n",
      "global step 4900 learning rate 0.7290 step-time 0.66 perplexity 1.16\n",
      "global step 5000 learning rate 0.7290 step-time 0.68 perplexity 1.16\n",
      "global step 5100 learning rate 0.7290 step-time 0.67 perplexity 1.14\n",
      "global step 5200 learning rate 0.7290 step-time 0.66 perplexity 1.06\n",
      "global step 5300 learning rate 0.7290 step-time 0.66 perplexity 1.17\n",
      "global step 5400 learning rate 0.6561 step-time 0.71 perplexity 1.07\n",
      "global step 5500 learning rate 0.6561 step-time 0.68 perplexity 1.15\n",
      "global step 5600 learning rate 0.6561 step-time 0.70 perplexity 1.06\n",
      "global step 5700 learning rate 0.6561 step-time 0.69 perplexity 1.05\n",
      "global step 5800 learning rate 0.6561 step-time 0.65 perplexity 1.24\n",
      "global step 5900 learning rate 0.5905 step-time 0.68 perplexity 1.09\n",
      "global step 6000 learning rate 0.5905 step-time 0.65 perplexity 1.15\n",
      "global step 6100 learning rate 0.5905 step-time 0.66 perplexity 1.12\n",
      "global step 6200 learning rate 0.5905 step-time 0.69 perplexity 1.16\n",
      "global step 6300 learning rate 0.5314 step-time 0.65 perplexity 1.05\n",
      "global step 6400 learning rate 0.5314 step-time 0.70 perplexity 1.05\n",
      "global step 6500 learning rate 0.5314 step-time 0.66 perplexity 1.03\n",
      "global step 6600 learning rate 0.5314 step-time 0.68 perplexity 1.04\n",
      "global step 6700 learning rate 0.5314 step-time 0.65 perplexity 1.11\n",
      "global step 6800 learning rate 0.4783 step-time 0.67 perplexity 1.06\n",
      "global step 6900 learning rate 0.4783 step-time 0.66 perplexity 1.09\n",
      "global step 7000 learning rate 0.4783 step-time 0.69 perplexity 1.04\n",
      "global step 7100 learning rate 0.4783 step-time 0.65 perplexity 1.03\n",
      "global step 7200 learning rate 0.4783 step-time 0.70 perplexity 1.04\n",
      "global step 7300 learning rate 0.4783 step-time 0.64 perplexity 1.03\n",
      "global step 7400 learning rate 0.4783 step-time 0.69 perplexity 1.03\n",
      "global step 7500 learning rate 0.4783 step-time 0.61 perplexity 1.02\n",
      "global step 7600 learning rate 0.4783 step-time 0.70 perplexity 1.03\n",
      "global step 7700 learning rate 0.4783 step-time 0.67 perplexity 1.02\n",
      "global step 7800 learning rate 0.4783 step-time 0.69 perplexity 1.02\n",
      "global step 7900 learning rate 0.4783 step-time 0.64 perplexity 1.02\n",
      "global step 8000 learning rate 0.4783 step-time 0.67 perplexity 1.02\n",
      "global step 8100 learning rate 0.4783 step-time 0.67 perplexity 1.02\n",
      "global step 8200 learning rate 0.4305 step-time 0.68 perplexity 1.02\n",
      "global step 8300 learning rate 0.3874 step-time 0.67 perplexity 1.02\n",
      "global step 8400 learning rate 0.3487 step-time 0.68 perplexity 1.02\n",
      "global step 8500 learning rate 0.3487 step-time 0.66 perplexity 1.02\n",
      "global step 8600 learning rate 0.3487 step-time 0.69 perplexity 1.02\n",
      "global step 8700 learning rate 0.3487 step-time 0.68 perplexity 1.02\n",
      "global step 8800 learning rate 0.3138 step-time 0.67 perplexity 1.02\n",
      "global step 8900 learning rate 0.2824 step-time 0.66 perplexity 1.03\n",
      "global step 9000 learning rate 0.2542 step-time 0.69 perplexity 1.02\n",
      "global step 9100 learning rate 0.2542 step-time 0.66 perplexity 1.02\n",
      "global step 9200 learning rate 0.2542 step-time 0.69 perplexity 1.02\n",
      "global step 9300 learning rate 0.2542 step-time 0.63 perplexity 1.02\n",
      "global step 9400 learning rate 0.2542 step-time 0.69 perplexity 1.02\n",
      "global step 9500 learning rate 0.2542 step-time 0.64 perplexity 1.02\n",
      "global step 9600 learning rate 0.2542 step-time 0.69 perplexity 1.02\n",
      "global step 9700 learning rate 0.2288 step-time 0.65 perplexity 1.02\n",
      "global step 9800 learning rate 0.2288 step-time 0.65 perplexity 1.01\n",
      "global step 9900 learning rate 0.2288 step-time 0.70 perplexity 1.02\n",
      "global step 10000 learning rate 0.2288 step-time 0.64 perplexity 1.01\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model.\n",
    "    print(\"Creating %d layers of %d units.\" % (num_layers, model_size))\n",
    "    model = create_model(sess, False)\n",
    "\n",
    "    # Read data into buckets and compute their sizes.\n",
    "    train_set = generate_data(max_train_data_size)\n",
    "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "    print(\"train total size %d\" % train_total_size)\n",
    "\n",
    "    # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "    # the size if i-th training bucket, as used later.\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    while current_step < num_steps:\n",
    "      # Choose a bucket according to data distribution. We pick a random number\n",
    "      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "      random_number_01 = np.random.random_sample()\n",
    "      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                       if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          train_set, bucket_id)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, bucket_id, False)\n",
    "      step_time += (time.time() - start_time) / steps_per_checkpoint\n",
    "      loss += step_loss / steps_per_checkpoint\n",
    "      current_step += 1\n",
    "\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if current_step % steps_per_checkpoint == 0:\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "        # Save checkpoint and zero timer and loss.\n",
    "        checkpoint_path = os.path.join(train_dir, \"translate.ckpt\")\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        \n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from /tmp/translate.ckpt-10000\n",
      "eht kciuq nworb xof\n"
     ]
    }
   ],
   "source": [
    "def decode():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model and load parameters.\n",
    "    model = create_model(sess, True)\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n",
    "    sentence = \"the quick brown fox\"\n",
    "    \n",
    "    # Get token-ids for the input sentence.\n",
    "    token_ids = [char2id(x) for x in sentence]\n",
    "    # Which bucket does it belong to?\n",
    "    bucket_id = len(_buckets) - 1\n",
    "    for i, bucket in enumerate(_buckets):\n",
    "        if bucket[0] >= len(token_ids):\n",
    "            bucket_id = i\n",
    "            break\n",
    "        else:\n",
    "            logging.warning(\"Sentence truncated: %s\", sentence)\n",
    "\n",
    "    # Get a 1-element batch to feed the sentence to the model.\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "      {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "    # Get output logits for the sentence.\n",
    "    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, bucket_id, True)\n",
    "    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    # If there is an EOS symbol in outputs, cut them at that point.\n",
    "    if data_utils.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "    print(''.join([id2char(output) for output in outputs]))\n",
    "    \n",
    "decode()        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
